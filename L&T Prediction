# -*- coding: utf-8 -*-
"""
Created on Wed Apr 17 13:57:10 2019

@author: Akhil Reddy Thirumuru
"""

import os
os.chdir(r'H:\kaggle\LTFS')

#Importing the required libraries
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
from datetime import datetime
from datetime import timedelta
#from calendar import isleap
from sklearn.linear_model import LogisticRegression

#importing the training and test sets
train = pd.read_csv('train.csv', header = 0)
test = pd.read_csv('test_bqCt9Pv.csv', header = 0)

#Checking if there is a bias in target variable distribution
train["loan_default"].value_counts() 
"""There is 3.5 times as many goods as bad - biased"""

#Data Basics
head = train.head()
des = train.describe()
train.groupby('loan_default').hist()

#Basic charts to see how the data is distributed accross variables - univariate
cols = ['manufacturer_id', 'Employment.Type', 'State_ID', 'Aadhar_flag',
        'PAN_flag', 'VoterID_flag', 'Driving_flag', 'Passport_flag']
fig = plt.figure()
plt.title('Count plots by features')
for i in (range(len(cols))):
    ax = fig.add_subplot(2,4,i+1)
    sns.countplot(data = train, x = cols[i],  hue = "loan_default")


"""There are a few features which by themselves are useless. For instance, 
Date birth - Age of the person
Disbursal date is there - can construct a feature called time on books which is more relevant"""
#Feature Engineering
train.columns
#Converting date columns to more useful numeric features
train['Date.of.Birth'] = pd.to_datetime(train['Date.of.Birth'], format='%d-%m-%y')
train['Age_of_person'] = (datetime.today() - train['Date.of.Birth'])/timedelta(days = 365.25)

train['DisbursalDate'] = pd.to_datetime(train['DisbursalDate'], format='%d-%m-%y')
train['Months_on_book'] = (datetime.today() - train['DisbursalDate'])/timedelta(days = 30)

train["AVERAGE_ACCT_AGE"] = (train['AVERAGE.ACCT.AGE'].str.extract("(\d+)").astype('int64') * 12) + train['AVERAGE.ACCT.AGE'].str.extract("yrs (\d+)").astype('int64')
train["CREDIT_HISTORY_LENGTH"] = (train['CREDIT.HISTORY.LENGTH'].str.extract("(\d+)").astype('int64') * 12) + train['CREDIT.HISTORY.LENGTH'].str.extract("yrs (\d+)").astype('int64')
train = train.drop(['Date.of.Birth', 'DisbursalDate', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], axis=1)


#Lets look at the categorical columns 'Employment.Type'- and 'PERFORM_CNS.SCORE.DESCRIPTION'
sns.countplot(data = train, x = "Employment.Type",  hue = "loan_default")
train = pd.concat([train.drop('Employment.Type', axis=1), pd.get_dummies(train['Employment.Type'])], axis=1) #creating dummies and dropping original column

sns.countplot(data = train, y = "PERFORM_CNS.SCORE.DESCRIPTION",  hue = "loan_default")
train["PERFORM_CNS.SCORE.DESCRIPTION"].value_counts()
"""Lets categorize this into low, medium, high risk and not scored - just intuition"""
def risksegment(x):
    if (x['PERFORM_CNS.SCORE.DESCRIPTION']) == '':
        return "Missing"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "C-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "A-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "D-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "B-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "F-Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "E-Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "G-Low Risk")):
        return "Low"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "H-Medium Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "I-Medium Risk")):
        return "Medium"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "M-Very High Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "K-High Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "J-High Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "L-Very High Risk")):
        return "High"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "No Bureau History Available") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: Sufficient History Not Available") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: Not Enough Info available on the customer") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: No Activity seen on the customer (Inactive)") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: No Updates available in last 36 months") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: Only a Guarantor") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: More than 50 active Accounts found")):
        return "Not_scored"
train["risk_level"] = train.apply(risksegment, axis =1)
train = train.drop("PERFORM_CNS.SCORE.DESCRIPTION", axis = 1)
train = pd.concat([train.drop('risk_level', axis=1), pd.get_dummies(train['risk_level'])], axis=1) #creating dummies and dropping original column

#Correlations
corr = train.corr()
xticks = train.columns
yticks = train.index
sns.heatmap(corr, cmap='RdYlGn', linewidth=0.3, annot = True, fmt = ".1f")
plt.yticks(rotation = 0)
plt.xticks(rotation = 90)
"""There are some highly correlated variables. Have to look at each variable and decide which one to keep and which one's to eliminate
Otherwise we can apply a model evaluate a feature score to remove it- going with the second one."""

#Applying logistic regression model
from sklearn.linear_model import LogisticRegression
X = train[[x for x in train.columns if x not in ["UniqueID", "loan_default"]]]
y = train['loan_default']
logreg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)
logreg.score(X,y)
logreg.predict_proba(X) 
logreg.coef_
logreg.intercept_

print(list(zip(logreg.coef_, train[[x for x in train.columns if x not in ["UniqueID"]]].columns)))
coef_dict = {}
for coef, feat in zip(logreg.coef_,train[[x for x in train.columns if x not in ["UniqueID", "loan_default"]]].columns):
    coef_dict[feat] = coef

#Trying out stability selection
from sklearn.linear_model import RandomizedLogisticRegression
randomized_logistic = RandomizedLogisticRegression() 
predictors = train[[x for x in train.columns if x not in ["UniqueID", "loan_default"]]].columns
randomized_logistic.fit(train[predictors], train['loan_default'])
print ("Features sorted by their score:")
print (sorted(zip(map(lambda x: round(x, 4), randomized_logistic.scores_), predictors), reverse=True))
logreg_scores = pd.DataFrame(sorted(zip(map(lambda x: round(x, 4), randomized_logistic.scores_), predictors), reverse=True))
logreg_scores.columns = ['score', 'feature']
"""Looking log_reg scores it the clearly evident that the 22 features with score > 0.675 have more weight on the model
selecting only those 22"""
features_selected = logreg_scores[logreg_scores["score"] > 0.65]["feature"]

"""performing another logistic regression with selected features and lets try sampling since the data is biased"""
X1 = train[features_selected]
number_records_defualt = len(train[train.loan_default == 1])
default_indices = np.array(train[train.loan_default == 1].index)
non_defualt_indices = train[train.loan_default == 0].index

random_non_defualt_indices = np.random.choice(non_defualt_indices, number_records_defualt, replace = False)
random_non_defualt_indices = np.array(random_non_defualt_indices)

# concatenate everything together
sampled_indices = np.concatenate([default_indices,random_non_defualt_indices])

# Under sample dataset
sampled_data = train.iloc[sampled_indices,:]
X_sampled = sampled_data.ix[:, sampled_data[[x for x in sampled_data.columns if x not in ['UniqueID', 'loan_default']]].columns]
y_sampled = sampled_data.ix[:, sampled_data.columns == 'loan_default']

# Showing ratio
print("Percentage of normal transactions: ", len(sampled_data[sampled_data.loan_default == 0])/len(sampled_data))
print("Percentage of fraud transactions: ", len(sampled_data[sampled_data.loan_default == 1])/len(sampled_data))
print("Total number of transactions in resampled data: ", len(sampled_data))

#Dividing into test train 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size = 0.25, random_state = 0)

#importing logistic regression
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import KFold, cross_val_score
from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report, precision_score

def printing_Kfold_scores(x_train_data,y_train_data,c_param_range,k):
    fold = KFold(len(y_train_data),k,shuffle=False) 
    
    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score', 'Mean '])
    k = 0
    for c_param in c_param_range:
        print('C parameter: ', c_param)
        
        recall_accs = []
        for iteration, indices in enumerate(fold,start=1):
            train_indices = indices[0]
            test_indices = indices[1]
            # Call the logistic regression model with a certain C parameter
            lr = LogisticRegression(C = c_param, penalty = 'l1')
            
            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model
            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]
            lr.fit(x_train_data.iloc[train_indices,:],y_train_data.iloc[train_indices,:].values.ravel())

            # Predict values using the test indices in the training data
            y_pred_undersample = lr.predict(x_train_data.iloc[test_indices,:].values)

            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter
            recall_acc = recall_score(y_train_data.iloc[test_indices,:].values,y_pred_undersample)
            recall_accs.append(recall_acc)
            print('Iteration ', iteration,': recall score = ', recall_acc)

        # The mean value of those recall scores is the metric we want to save and get hold of.
        results_table.ix[k,'Mean recall score'] = np.mean(recall_accs)
        results_table.ix[k,'C_parameter'] = c_param
        k += 1
        print('')
        print('Mean recall score ', np.mean(recall_accs))
        print('')

    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']
    
    print('Best model w/ regards to recall has C parameter ', best_c)
    
    return best_c
best = printing_Kfold_scores(X_train[features_selected],y_train,[0.0001,0.001,0.01,0.1,1,10],10)

import matplotlib.pyplot as plt
# Call the logistic regression model with the best C parameter w/ to recall
lr = LogisticRegression(C = 0.01, penalty = 'l1')

lr.fit(X_train[features_selected],y_train.values.ravel())
y_test_pred = lr.predict(X_test[features_selected])
    
precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)
plt.plot(recall, precision, color='navy', label='Precision-Recall curve')

#Measuring model performace
score = lr.score(X_test[features_selected], y_test)
print(lr.score(X_test[features_selected], y_test)*100) #accuracy for 25% is 78.361 

#Confusion matrix
from sklearn import metrics
cm = metrics.confusion_matrix(y_test, y_test_pred)
print(cm)
#visualising confusion matrix
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r', cbar = False);
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(score)
plt.title(all_sample_title, size = 15);

         
         
         
         
         
"""Step2 feature reduction"""         
#Selecting features with business sense
#features_selected2 = ['ltv', 'PRI.OVERDUE.ACCTS', 'PRI.ACTIVE.ACCTS', 'NO.OF_INQUIRIES', 'Months_on_book', 
#'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS', 'CREDIT_HISTORY_LENGTH', 'disbursed_amount', 'PRI.CURRENT.BALANCE', 
#'PRI.NO.OF.ACCTS', 'Medium', 'High', 'Low', 'manufacturer_id', 'branch_id', 'State_ID', 'Self employed', 'Salaried']
#Feature importance
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
ETC = ExtraTreesClassifier()
ETC.fit(X,y)
print(ETC.feature_importances_) #using inbuilt class feature_importances_ of the Tree based classifiers
feature_importances = pd.DataFrame(ETC.feature_importances_, index = X.columns).reset_index()
feature_importances.columns = ['Variable','Score']
feature_importances.nlargest(25, 'Score').plot(kind = 'barh')  

"""Try top 8 and top 11"""
features_selected2 =  feature_importances.nlargest(8, 'Score').Variable.tolist()

#Run the logistic regression model again with this features
lr = LogisticRegression(C = 0.01, penalty = 'l1')

lr.fit(X_train[features_selected2],y_train.values.ravel())
y_test_pred = lr.predict(X_test[features_selected2])
    
precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)
#plt.plot(recall, precision, color='navy', label='Precision-Recall curve')

#Measuring model performace
score = lr.score(X_test[features_selected2], y_test)
print(lr.score(X_test[features_selected2], y_test)*100)

#Confusion matrix
from sklearn import metrics
cm = metrics.confusion_matrix(y_test, y_test_pred)
print(cm)
#visualising confusion matrix
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r', cbar = False);
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(score)
plt.title(all_sample_title, size = 15);





logreg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(x_sel_train, y_train)
logreg.score(x_sel_train,y_train)

predictions = logreg.predict(x_sel_test)
#Measuring model performace
score = logreg.score(x_sel_test, y_test)
print(logreg.score(x_sel_test, y_test)*100) #accuracy for 25% is 78.361 

#report
from sklearn import metrics
cr = metrics.classification_report(y_test, predictions) #The report says that the prediction accuracy for 1 is too less 


#Confusion matrix
from sklearn import metrics
cm = metrics.confusion_matrix(y_test, predictions)
print(cm)
#visualising confusion matrix
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r', cbar = False);
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(score)
plt.title(all_sample_title, size = 15);

         
#Model selection
#Model selection
from sklearn import cross_validation as cv
from sklearn import tree
from sklearn import metrics
from sklearn import ensemble
from sklearn import linear_model 
from sklearn import naive_bayes 
import xgboost as  xgb
skf = cv.StratifiedKFold(y, n_folds=3, shuffle=True)
score_metric = 'roc_auc'
scores = {}

def score_model(model):
    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)

scores['tree'] = score_model(tree.DecisionTreeClassifier()) 
scores['extra_tree'] = score_model(ensemble.ExtraTreesClassifier())
scores['forest'] = score_model(ensemble.RandomForestClassifier())
scores['ada_boost'] = score_model(ensemble.AdaBoostClassifier())
scores['bagging'] = score_model(ensemble.BaggingClassifier())
scores['grad_boost'] = score_model(ensemble.GradientBoostingClassifier())
scores['ridge'] = score_model(linear_model.RidgeClassifier())
scores['passive'] = score_model(linear_model.PassiveAggressiveClassifier())
scores['sgd'] = score_model(linear_model.SGDClassifier())
scores['gaussian'] = score_model(naive_bayes.GaussianNB())
scores['xgboost'] = score_model(xgb.XGBClassifier())

#Print the model scores
model_scores = pd.DataFrame(scores).mean()
model_scores.sort_values(ascending=False)
model_scores.to_csv('model_scores.csv', index=False)
print('Model scores\n{}'.format(model_scores))
       








#xgboost implementation
import xgboost as xgb   
from xgboost import XGBClassifier
from xgboost.sklearn import XGBClassifier
from sklearn import cross_validation, metrics   #Additional scklearn functions
from sklearn.grid_search import GridSearchCV   #Perforimg grid search
import matplotlib.pylab as plt

def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):
    
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain["loan_default"].values)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
            metrics='auc', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain['loan_default'],eval_metric='auc')
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])
    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]
        
    #Print model report:
    print ("\nModel Report")
    print ("Accuracy : %.4g" % metrics.accuracy_score(dtrain['loan_default'].values, dtrain_predictions))
    print ("AUC Score (Train): %f" % metrics.roc_auc_score(dtrain['loan_default'], dtrain_predprob))
    print("Best Iteration: {}".format(alg._Booster.best_iteration))
    
    feat_imp = pd.Series(alg._Booster.get_fscore()).sort_values(ascending=False)
    feat_imp.plot(kind='bar', title='Feature Importances')
    plt.ylabel('Feature Importance Score')
    
#    make predictions for test data
#    y_pred = alg.predict(dtest)
#    predictions = pd.DataFrame([round(value) for value in y_pred])
#    return predictions


xgb1 = XGBClassifier(learning_rate = 0.1, n_estimators = 349, max_depth = 5, min_child_weight = 1, gamma = 0, 
                     subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)
modelfit(xgb1, train, features_selected)    

#looping xgb for best aoc
import sys
old_stdout = sys.stdout
log_file = open(r"xgboost_diag.txt","w")
sys.stdout = log_file
for i in np.arange(3,10,2):
    for j in np.arange(1,10,2):
        print("max_depth: {}".format(i))
        print("min_child_weight: {}".format(j))
        xgb_loop = XGBClassifier(learning_rate = 0.1, n_estimators = 349, max_depth = i, min_child_weight = j, gamma = 0, 
                     subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)
        modelfit(xgb_loop, train, features_selected2)
sys.stdout = old_stdout
log_file.close()   

"""The best parameters are as follows   
max_depth: 9
min_child_weight: 1
Model Report
Accuracy : 0.7902
AUC Score (Train): 0.778880
Best Iteration: 103
max_depth: 9
min_child_weight: 3"""


#Using the best parameters
xgb3 = XGBClassifier(learning_rate = 0.1, n_estimators = 349, max_depth = i, min_child_weight = j, gamma = 0, 
                     subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)
from sklearn.model_selection import train_test_split
x_train, x_test = train_test_split(train, test_size = 0.25, random_state = 0)
res = [x for x in test.columns if x not in ["ID_code"]]    
test_pred = modelfit(xgb2, x_train, x_test[features_selected], features_selected)

final_pred = modelfit(xgb2, train, test[features_selected], features_selected)

final_pred1 = final_pred
final_pred1["ID_code"] = test["ID_code"] 
final_pred1 = final_pred1.rename({0: "target"}, axis = 1)
final_pred1 = final_pred1[["ID_code", "target"]]
final_pred1.to_csv('predictions.csv')
final_pred[0].value_counts()
train["target"].value_counts()  
