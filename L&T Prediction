

# ## Changing the current working directory

import os
os.chdir(r'H:\kaggle\LTFS')


# ## Importing the required libraries


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
from datetime import datetime
from datetime import timedelta
#from calendar import isleap
from sklearn.linear_model import LogisticRegression


# # Importing the training and test sets


train = pd.read_csv('train.csv', header = 0)
test = pd.read_csv('test_bqCt9Pv.csv', header = 0)


# ## Checking if there is a bias in target variable distribution
# There is 3.5 times as many goods as bad - biased


train["loan_default"].value_counts() 


# ## Data Basics
train.head()


train.describe()


# ## Basic charts to see how the data is distributed
cols = ['manufacturer_id', 'Employment.Type', 'State_ID', 'Aadhar_flag',
        'PAN_flag', 'VoterID_flag', 'Driving_flag', 'Passport_flag']
fig = plt.figure()
plt.title('Count plots by features')
for i in (range(len(cols))):
    #ax = fig.add_subplot(2,4,i+1)
    sns.countplot(data = train, x = cols[i],  hue = "loan_default")
    plt.show()


# ## Feature Engineering
# There are a few features which by themselves are useless. For instance, 
# Date birth - Age of the person
# Disbursal date is there - can construct a feature called time on books which is more relevant
#Converting date columns to more useful numeric features
train['Date.of.Birth'] = pd.to_datetime(train['Date.of.Birth'], format='%d-%m-%y')
train['Age_of_person'] = (datetime.today() - train['Date.of.Birth'])/timedelta(days = 365.25)

train['DisbursalDate'] = pd.to_datetime(train['DisbursalDate'], format='%d-%m-%y')
train['Months_on_book'] = (datetime.today() - train['DisbursalDate'])/timedelta(days = 30)

train["AVERAGE_ACCT_AGE"] = (train['AVERAGE.ACCT.AGE'].str.extract("(\d+)").astype('int64') * 12) + train['AVERAGE.ACCT.AGE'].str.extract("yrs (\d+)").astype('int64')
train["CREDIT_HISTORY_LENGTH"] = (train['CREDIT.HISTORY.LENGTH'].str.extract("(\d+)").astype('int64') * 12) + train['CREDIT.HISTORY.LENGTH'].str.extract("yrs (\d+)").astype('int64')
train = train.drop(['Date.of.Birth', 'DisbursalDate', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], axis=1)


# ### Lets look at the categorical columns 'Employment.Type'- and 'PERFORM_CNS.SCORE.DESCRIPTION'
sns.countplot(data = train, x = "Employment.Type",  hue = "loan_default")
train = pd.concat([train.drop('Employment.Type', axis=1), pd.get_dummies(train['Employment.Type'])], axis=1) #creating dummies and dropping original column


sns.countplot(data = train, y = "PERFORM_CNS.SCORE.DESCRIPTION",  hue = "loan_default")
train["PERFORM_CNS.SCORE.DESCRIPTION"].value_counts()



def risksegment(x):
    if (x['PERFORM_CNS.SCORE.DESCRIPTION']) == '':
        return "Missing"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "C-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "A-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "D-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "B-Very Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "F-Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "E-Low Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "G-Low Risk")):
        return "Low"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "H-Medium Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "I-Medium Risk")):
        return "Medium"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "M-Very High Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "K-High Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "J-High Risk") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "L-Very High Risk")):
        return "High"
    elif ((x['PERFORM_CNS.SCORE.DESCRIPTION'] == "No Bureau History Available") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: Sufficient History Not Available") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: Not Enough Info available on the customer") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: No Activity seen on the customer (Inactive)") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: No Updates available in last 36 months") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: Only a Guarantor") | (x['PERFORM_CNS.SCORE.DESCRIPTION'] == "Not Scored: More than 50 active Accounts found")):
        return "Not_scored"
train["risk_level"] = train.apply(risksegment, axis =1)
train = train.drop("PERFORM_CNS.SCORE.DESCRIPTION", axis = 1)
train = pd.concat([train.drop('risk_level', axis=1), pd.get_dummies(train['risk_level'])], axis=1) #creating dummies and dropping original column


# ## Correlations
corr = train.corr()
xticks = train.columns
yticks = train.index
plt.figure(figsize=(18, 18))
sns.heatmap(corr, cmap='RdYlGn', linewidth=0.3, annot = True, fmt = ".1f")
plt.yticks(rotation = 0)
plt.xticks(rotation = 90)
plt.show()


# "There are some highly correlated variables. Have to look at each variable and decide which one to keep and which one's to eliminate
# Otherwise we can apply a model evaluate a feature score to remove it- going with the second one."

# ## Applying Basic logistic Regressions

from sklearn.linear_model import LogisticRegression
X = train[[x for x in train.columns if x not in ["UniqueID", "loan_default"]]]
y = train['loan_default']
logreg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)
print(logreg.score(X,y))
#print(logreg.predict_proba(X)) 
#print(logreg.coef_)
print(logreg.intercept_)
print(list(zip(logreg.coef_, train[[x for x in train.columns if x not in ["UniqueID"]]].columns)))
coef_dict = {}
for coef, feat in zip(logreg.coef_,train[[x for x in train.columns if x not in ["UniqueID", "loan_default"]]].columns):
    coef_dict[feat] = coef


# ## Feature Selection

# ### Trying out stability selection
from sklearn.linear_model import RandomizedLogisticRegression
randomized_logistic = RandomizedLogisticRegression() 
predictors = train[[x for x in train.columns if x not in ["UniqueID", "loan_default"]]].columns
randomized_logistic.fit(train[predictors], train['loan_default'])
print ("Features sorted by their score:")
#print (sorted(zip(map(lambda x: round(x, 4), randomized_logistic.scores_), predictors), reverse=True))
logreg_scores = pd.DataFrame(sorted(zip(map(lambda x: round(x, 4), randomized_logistic.scores_), predictors), reverse=True))
logreg_scores.columns = ['score', 'feature']
logreg_scores.sort_values('score', ascending=False)

"""Looking log_reg scores it the clearly evident that the 22 features with score > 0.675 have more weight on the model
selecting only those 22"""
features_selected = logreg_scores[logreg_scores["score"] > 0.65]["feature"]


# ## Performing another logistic regression with selected features and lets try sampling since the data is biased
X1 = train[features_selected]
number_records_defualt = len(train[train.loan_default == 1])
default_indices = np.array(train[train.loan_default == 1].index)
non_defualt_indices = train[train.loan_default == 0].index

random_non_defualt_indices = np.random.choice(non_defualt_indices, number_records_defualt, replace = False)
random_non_defualt_indices = np.array(random_non_defualt_indices)

# concatenate everything together
sampled_indices = np.concatenate([default_indices,random_non_defualt_indices])

# Under sample dataset
sampled_data = train.iloc[sampled_indices,:]
X_sampled = sampled_data.ix[:, sampled_data[[x for x in sampled_data.columns if x not in ['UniqueID', 'loan_default']]].columns]
y_sampled = sampled_data.ix[:, sampled_data.columns == 'loan_default']

# Showing ratio
print("Percentage of normal transactions: ", len(sampled_data[sampled_data.loan_default == 0])/len(sampled_data))
print("Percentage of fraud transactions: ", len(sampled_data[sampled_data.loan_default == 1])/len(sampled_data))
print("Total number of transactions in resampled data: ", len(sampled_data))

#Dividing into test train 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size = 0.25, random_state = 0)

#importing logistic regression
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import KFold, cross_val_score
from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report, precision_score

def printing_Kfold_scores(x_train_data,y_train_data,c_param_range,k):
    fold = KFold(len(y_train_data),k,shuffle=False) 
    
    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score', 'Mean '])
    k = 0
    for c_param in c_param_range:
        print('C parameter: ', c_param)
        
        recall_accs = []
        for iteration, indices in enumerate(fold,start=1):
            train_indices = indices[0]
            test_indices = indices[1]
            # Call the logistic regression model with a certain C parameter
            lr = LogisticRegression(C = c_param, penalty = 'l1')
            
            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model
            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]
            lr.fit(x_train_data.iloc[train_indices,:],y_train_data.iloc[train_indices,:].values.ravel())

            # Predict values using the test indices in the training data
            y_pred_undersample = lr.predict(x_train_data.iloc[test_indices,:].values)

            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter
            recall_acc = recall_score(y_train_data.iloc[test_indices,:].values,y_pred_undersample)
            recall_accs.append(recall_acc)
            print('Iteration ', iteration,': recall score = ', recall_acc)

        # The mean value of those recall scores is the metric we want to save and get hold of.
        results_table.ix[k,'Mean recall score'] = np.mean(recall_accs)
        results_table.ix[k,'C_parameter'] = c_param
        k += 1
        print('')
        print('Mean recall score ', np.mean(recall_accs))
        print('')

    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']
    
    print('Best model w/ regards to recall has C parameter ', best_c)
    
    return best_c
best = printing_Kfold_scores(X_train[features_selected],y_train,[0.0001,0.001,0.01,0.1,1,10],10)


import matplotlib.pyplot as plt
# Call the logistic regression model with the best C parameter w/ to recall
lr = LogisticRegression(C = 0.01, penalty = 'l1')

lr.fit(X_train[features_selected],y_train.values.ravel())
y_test_pred = lr.predict(X_test[features_selected])
    
precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)
plt.plot(recall, precision, color='navy', label='Precision-Recall curve')

#Measuring model performace
score = lr.score(X_test[features_selected], y_test)
print(lr.score(X_test[features_selected], y_test)*100) #accuracy for 25% is 78.361 

#Confusion matrix
from sklearn import metrics
cm = metrics.confusion_matrix(y_test, y_test_pred)
print(cm)
#visualising confusion matrix
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r', cbar = False);
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(score)
plt.title(all_sample_title, size = 15);
plt.show()


# ## Step2 feature reduction
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
ETC = ExtraTreesClassifier()
ETC.fit(X,y)
print(ETC.feature_importances_) #using inbuilt class feature_importances_ of the Tree based classifiers
feature_importances = pd.DataFrame(ETC.feature_importances_, index = X.columns).reset_index()
feature_importances.columns = ['Variable','Score']
feature_importances.nlargest(25, 'Score').plot(kind = 'barh')  

"""Try top 8 and top 11"""
features_selected2 =  feature_importances.nlargest(8, 'Score').Variable.tolist()

#Run the logistic regression model again with this features
lr = LogisticRegression(C = 0.01, penalty = 'l1')

lr.fit(X_train[features_selected2],y_train.values.ravel())
y_test_pred = lr.predict(X_test[features_selected2])
    
precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)
#plt.plot(recall, precision, color='navy', label='Precision-Recall curve')

#Measuring model performace
score = lr.score(X_test[features_selected2], y_test)
print(lr.score(X_test[features_selected2], y_test)*100)

#Confusion matrix
from sklearn import metrics
cm = metrics.confusion_matrix(y_test, y_test_pred)
print(cm)
#visualising confusion matrix
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r', cbar = False);
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(score)
plt.title(all_sample_title, size = 15);
plt.show()


# ### The Accuracy and AUC don't show much improvement. Trying XGBoost

# ## xgboost

# In[58]:

import xgboost as xgb   
from xgboost import XGBClassifier
from xgboost.sklearn import XGBClassifier
from sklearn import cross_validation, metrics   #Additional scklearn functions
from sklearn.grid_search import GridSearchCV   #Perforimg grid search
import matplotlib.pylab as plt

def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):
    
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain["loan_default"].values)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
            metrics='auc', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain['loan_default'],eval_metric='auc')
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])
    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]
        
    #Print model report:
    print ("\nModel Report")
    print ("Accuracy : %.4g" % metrics.accuracy_score(dtrain['loan_default'].values, dtrain_predictions))
    print ("AUC Score (Train): %f" % metrics.roc_auc_score(dtrain['loan_default'], dtrain_predprob))
    print("Best Iteration: {}".format(alg._Booster.best_iteration))
    
    feat_imp = pd.Series(alg._Booster.get_fscore()).sort_values(ascending=False)
    feat_imp.plot(kind='bar', title='Feature Importances')
    plt.ylabel('Feature Importance Score')
xgb1 = XGBClassifier(learning_rate = 0.1, n_estimators = 349, max_depth = 5, min_child_weight = 1, gamma = 0, 
                     subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)
modelfit(xgb1, train, features_selected)    


# #### looping xgb for best auc
for i in np.arange(3,10,2):
    for j in np.arange(1,10,2):
        print("max_depth: {}".format(i))
        print("min_child_weight: {}".format(j))
        xgb_loop = XGBClassifier(learning_rate = 0.1, n_estimators = 349, max_depth = i, min_child_weight = j, gamma = 0, 
                     subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)
        modelfit(xgb_loop, train, features_selected2)


# ## Using the best parameters for xgboost
xgb3 = XGBClassifier(learning_rate = 0.1, n_estimators = 106, max_depth = 9, min_child_weight = 5, gamma = 0, 
                     subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)
from sklearn.model_selection import train_test_split
x_train, x_test = train_test_split(train, test_size = 0.25, random_state = 0)
res = [x for x in test.columns if x not in ["ID_code"]]    
test_pred = modelfit(xgb2, x_train, x_test[features_selected], features_selected)

final_pred = modelfit(xgb2, train, test[features_selected], features_selected)

final_pred1 = final_pred
final_pred1["ID_code"] = test["ID_code"] 
final_pred1 = final_pred1.rename({0: "target"}, axis = 1)
final_pred1 = final_pred1[["ID_code", "target"]]
final_pred1.to_csv('predictions.csv')
final_pred[0].value_counts()
train["target"].value_counts()  

